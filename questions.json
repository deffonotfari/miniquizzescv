[
  {
    "section": "digital-images-and-image-processing",
    "question": "A grayscale digital image defined as I: Z² → Z means:",
    "choices": {
      "A": "Each integer spatial coordinate (x,y) maps to one discrete intensity value.",
      "B": "Each real coordinate maps to a real intensity value.",
      "C": "Each pixel stores three colour values.",
      "D": "The image is a continuous function over space."
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A colour digital image defined as I: Z² → Z³ implies that:",
    "choices": {
      "A": "The image has three spatial dimensions (x,y,z).",
      "B": "Each spatial coordinate maps to three discrete values (e.g., R,G,B).",
      "C": "The image must be a video (x,y,t).",
      "D": "Each pixel maps to a single floating-point value."
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Before digitisation, the light field reaching a camera sensor is best described as:",
    "choices": {
      "A": "A spatially discrete digital signal",
      "B": "A spatially continuous analogue signal",
      "C": "A binary signal",
      "D": "A compressed signal"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "In a digital camera pipeline, spatial sampling happens primarily at the:",
    "choices": {
      "A": "JPEG encoder",
      "B": "Image sensor (sampling grid of photosites)",
      "C": "Histogram equalisation step",
      "D": "Display gamma curve"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "In a digital camera pipeline, quantisation happens primarily at the:",
    "choices": {
      "A": "Lens",
      "B": "Colour filter array (CFA)",
      "C": "Analogue-to-digital converter (ADC)",
      "D": "Bayer demosaicing algorithm"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Spatial sampling converts:",
    "choices": {
      "A": "Continuous intensities into discrete intensity levels",
      "B": "Continuous spatial coordinates into discrete pixel locations",
      "C": "RGB into HSV",
      "D": "A 2D signal into a 1D signal"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Quantisation converts:",
    "choices": {
      "A": "Continuous spatial coordinates into pixel indices",
      "B": "Discrete pixel indices into continuous coordinates",
      "C": "Continuous signal amplitudes into a finite set of discrete levels",
      "D": "A colour image into grayscale"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "What does quantisation do?",
    "choices": {
      "A": "Continuous spatial coordinates into pixel indices",
      "B": "Discrete pixel indices into continuous coordinates",
      "C": "Continuous signal amplitudes into a finite set of discrete levels",
      "D": "It limits the values a pixel can have to a finite set"
    },
    "answer": "D"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A digital image stored on a computer is most accurately described as:",
    "choices": {
      "A": "A continuous light field",
      "B": "A matrix/array of discrete pixel values",
      "C": "A set of real-valued functions only",
      "D": "A JPEG bitstream by definition"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A video signal I(x,y,t) differs from a 2D image I(x,y) because it:",
    "choices": {
      "A": "Has no spatial dimensions",
      "B": "Includes time as an additional independent variable",
      "C": "Cannot be digitised",
      "D": "Must be grayscale"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "In the notation I(x,y), (x,y) most directly refer to:",
    "choices": {
      "A": "Histogram bin and count",
      "B": "Spatial coordinates (pixel position)",
      "C": "Bit depth and dynamic range",
      "D": "Hue and saturation"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "An image stored with 1 bits per pixel can represent how many distinct intensity levels per channel?",
    "choices": {
      "A": "2",
      "B": "2",
      "C": "1",
      "D": "1"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "An image stored with 4 bits per pixel can represent how many distinct intensity levels per channel?",
    "choices": {
      "A": "16",
      "B": "8",
      "C": "16",
      "D": "8"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "An image stored with 8 bits per pixel can represent how many distinct intensity levels per channel?",
    "choices": {
      "A": "256",
      "B": "16",
      "C": "64",
      "D": "128"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "An image stored with 12 bits per pixel can represent how many distinct intensity levels per channel?",
    "choices": {
      "A": "4096",
      "B": "24",
      "C": "144",
      "D": "2048"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Increasing bit depth (with the same spatial sampling) most directly increases:",
    "choices": {
      "A": "Field of view",
      "B": "Dynamic range / intensity precision",
      "C": "Number of pixels",
      "D": "Frame rate"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Increasing the number of pixels (with the same bit depth) most directly increases:",
    "choices": {
      "A": "Spatial detail representation",
      "B": "Intensity precision",
      "C": "Quantisation levels",
      "D": "Colour gamut"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Quantisation error (banding) is most directly reduced by:",
    "choices": {
      "A": "Increasing bit depth",
      "B": "Decreasing image width",
      "C": "Using a larger σ in Gaussian smoothing",
      "D": "Converting RGB to CMYK"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Aliasing due to insufficient spatial sampling is primarily mitigated by:",
    "choices": {
      "A": "Higher PPI when printing",
      "B": "Higher spatial sampling (more sensor samples per area) and appropriate prefiltering",
      "C": "Higher bit depth only",
      "D": "Histogram equalisation"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Image resolution most fundamentally measures:",
    "choices": {
      "A": "File size",
      "B": "The amount of detail the image can represent",
      "C": "The number of colour models available",
      "D": "Compression ratio"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Pixel resolution refers to:",
    "choices": {
      "A": "Physical size of a pixel in millimetres",
      "B": "Image dimensions in pixels (width×height)",
      "C": "Number of intensity levels",
      "D": "Dynamic range"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Spatial resolution refers to:",
    "choices": {
      "A": "Pixels per inch on paper",
      "B": "Bits per pixel",
      "C": "Size represented by each pixel (e.g., mm/pixel)",
      "D": "Total number of pixels"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Pixel density is most commonly used to describe:",
    "choices": {
      "A": "Printing quality",
      "B": "Screens and sensors (pixels per unit length/area)",
      "C": "Histogram flatness",
      "D": "Convolution kernel size"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "PPI is most directly used when:",
    "choices": {
      "A": "Estimating affine transforms",
      "B": "Printing an image (mapping pixels to inches)",
      "C": "Computing gradients",
      "D": "Performing demosaicing"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Why does a Bayer colour filter array (CFA) typically have more green samples than red or blue?",
    "choices": {
      "A": "Green pixels store more bits",
      "B": "Human vision is more sensitive to green wavelengths",
      "C": "Green light is always brighter",
      "D": "It reduces JPEG artefacts"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Demosaicing is needed because:",
    "choices": {
      "A": "The lens produces a mosaic image",
      "B": "Each sensor photosite measures only one colour through the CFA",
      "C": "RGB images are stored as grayscale",
      "D": "RAW images contain no sensor data"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "RGB is an additive colour model because:",
    "choices": {
      "A": "Colours are produced by adding light intensities",
      "B": "Inks absorb wavelengths from white paper",
      "C": "It subtracts from black background",
      "D": "It uses hue as the main axis"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "CMYK is a subtractive colour model because:",
    "choices": {
      "A": "It adds light to black",
      "B": "It models ink absorption that subtracts light from white",
      "C": "It is identical to RGB",
      "D": "It is designed for sensors"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "How many distinct colours can 24-bit RGB represent (8 bits per channel)?",
    "choices": {
      "A": "2^8",
      "B": "2^16",
      "C": "2^24",
      "D": "24^2"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "BMP files are often large mainly because they are commonly stored as:",
    "choices": {
      "A": "Uncompressed raster data",
      "B": "Lossy DCT coefficients",
      "C": "Palette-only indices",
      "D": "Vector primitives"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "JPEG is best described as:",
    "choices": {
      "A": "Lossless compression",
      "B": "Lossy compression",
      "C": "Uncompressed storage",
      "D": "Palette-limited lossless compression"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "PNG is best described as:",
    "choices": {
      "A": "Always lossy",
      "B": "Lossless compression",
      "C": "Always palette-limited",
      "D": "Always 16-bit per channel"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "GIF is limited mainly because it typically supports:",
    "choices": {
      "A": "Only 256 colours (8-bit palette)",
      "B": "Only grayscale images",
      "C": "Only CMYK images",
      "D": "Only 1-bit images"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "TIFF is commonly used when you need:",
    "choices": {
      "A": "The smallest files possible",
      "B": "Flexible, often lossless/high-bit-depth storage for archiving/editing",
      "C": "Only web thumbnails",
      "D": "Only animations"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A 12-megapixel RGB image stored uncompressed at 24 bits/pixel requires approximately:",
    "choices": {
      "A": "12 MB",
      "B": "24 MB",
      "C": "36 MB",
      "D": "72 MB"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A brightness transformation J = f(I) is called position-independent because:",
    "choices": {
      "A": "It depends on (x,y) but not on intensity",
      "B": "It depends only on intensity values, not on pixel location",
      "C": "It depends only on location",
      "D": "It requires convolution"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A histogram of a grayscale image represents:",
    "choices": {
      "A": "The spatial arrangement of pixels",
      "B": "The frequency of intensity values",
      "C": "Edge orientation distribution",
      "D": "Pixel density"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "The cumulative distribution function (CDF) gives, for a given intensity value:",
    "choices": {
      "A": "The fraction of pixels with intensity ≤ that value",
      "B": "The gradient magnitude at that value",
      "C": "The kernel size needed for smoothing",
      "D": "The image width in pixels"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Contrast stretching with J = αI + β primarily aims to:",
    "choices": {
      "A": "Extend the dynamic range used by intensities",
      "B": "Reduce spatial resolution",
      "C": "Convert RGB to HSV",
      "D": "Compute edges"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Gamma correction applies a non-linear transform mainly to:",
    "choices": {
      "A": "Match human non-linear perception of brightness",
      "B": "Increase pixel count",
      "C": "Remove lens distortion",
      "D": "Make histograms uniform by definition"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Histogram equalisation mainly attempts to:",
    "choices": {
      "A": "Make the histogram roughly uniform over the dynamic range",
      "B": "Make the image binary",
      "C": "Preserve exact intensities",
      "D": "Increase PPI"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Adaptive histogram equalisation (e.g., CLAHE) differs from global equalisation because it:",
    "choices": {
      "A": "Uses a single global mapping",
      "B": "Improves contrast locally in regions",
      "C": "Works only on colour images",
      "D": "Is always identical to contrast stretching"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A geometric transformation x' = T(x) changes:",
    "choices": {
      "A": "Only intensity values",
      "B": "Pixel locations (spatial positions)",
      "C": "Bit depth only",
      "D": "Histogram bin counts only"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Inverse mapping is often used in image warping mainly to:",
    "choices": {
      "A": "Avoid holes (unassigned pixels) in the output image",
      "B": "Increase bit depth automatically",
      "C": "Remove compression artefacts",
      "D": "Make kernels separable"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Which interpolation uses the four nearest neighbours around a subpixel location?",
    "choices": {
      "A": "Nearest neighbour",
      "B": "Bilinear interpolation",
      "C": "Bicubic interpolation",
      "D": "Median interpolation"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A 2D affine transform in homogeneous coordinates is typically represented by a:",
    "choices": {
      "A": "2×2 matrix",
      "B": "3×3 matrix with last row [0 0 1]",
      "C": "4×4 matrix",
      "D": "1×3 vector"
    },
    "answer": "B"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "How many degrees of freedom (parameters) does a 2D affine transformation have?",
    "choices": {
      "A": "3",
      "B": "4",
      "C": "6",
      "D": "8"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A 2D projective transformation (homography) has how many degrees of freedom (with scale normalisation)?",
    "choices": {
      "A": "6",
      "B": "7",
      "C": "8",
      "D": "9"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "How many point correspondences are sufficient to estimate a 2D projective transform (homography) in general position?",
    "choices": {
      "A": "2",
      "B": "3",
      "C": "4",
      "D": "6"
    },
    "answer": "C"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Which statement is correct about RAW vs processed images?",
    "choices": {
      "A": "RAW typically stores minimally processed sensor measurements, while processed images apply steps like white balance and tone mapping",
      "B": "RAW is always smaller than JPEG",
      "C": "Processed images cannot be colour",
      "D": "RAW images cannot be displayed"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "A key reason to use float images during processing is to:",
    "choices": {
      "A": "Avoid clipping/quantisation during intermediate computations",
      "B": "Increase sensor pixel density",
      "C": "Increase PPI when printing",
      "D": "Make the image additive"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "If you print the same 3000×2000 image at higher PPI, the printed image becomes:",
    "choices": {
      "A": "Physically smaller on paper",
      "B": "Physically larger on paper",
      "C": "Higher bit depth",
      "D": "More pixels in the file"
    },
    "answer": "A"
  },
  {
    "section": "digital-images-and-image-processing",
    "question": "Which statement best distinguishes pixel resolution from spatial resolution?",
    "choices": {
      "A": "Pixel resolution is measured in pixels; spatial resolution is measured in physical units per pixel (e.g., mm/pixel)",
      "B": "Pixel resolution is measured in mm/pixel; spatial resolution in pixels",
      "C": "They mean exactly the same thing",
      "D": "Spatial resolution refers only to colour depth"
    },
    "answer": "A"
  },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is image filtering?",
      "choices": {
        "A": "Changing image resolution",
        "B": "Removing unwanted components or enhancing desired ones in an image",
        "C": "Transforming colour spaces",
        "D": "Estimating affine transformations"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is window-based image filtering?",
      "choices": {
        "A": "Filtering using histogram bins",
        "B": "Computing each output pixel from a neighbourhood defined by an N×N window",
        "C": "Filtering using Fourier transforms",
        "D": "Resizing the image"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "When is filtering considered linear?",
      "choices": {
        "A": "When the output is the median of the window",
        "B": "When output pixels are linear combinations of neighbourhood pixels with fixed weights",
        "C": "When kernel values change per pixel",
        "D": "When σ is large"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is a kernel in linear filtering?",
      "choices": {
        "A": "A histogram",
        "B": "A matrix of weights used to compute linear combinations of pixel values",
        "C": "A colour channel",
        "D": "A gradient map"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is convolution in image processing?",
      "choices": {
        "A": "A non-linear filtering method",
        "B": "The act of moving a linear kernel over an image to compute weighted sums",
        "C": "Histogram stretching",
        "D": "Colour normalisation"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Which property states that K * I = I * K?",
      "choices": {
        "A": "Associative",
        "B": "Distributive",
        "C": "Commutative",
        "D": "Non-linear"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Which property states that K1 * I + K2 * I = (K1 + K2) * I?",
      "choices": {
        "A": "Associative",
        "B": "Commutative",
        "C": "Distributive",
        "D": "Linear scaling"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is zero padding in edge handling?",
      "choices": {
        "A": "Removing border pixels",
        "B": "Extending the image using zeros",
        "C": "Wrapping image borders",
        "D": "Blurring edges"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What does kernel normalisation ensure?",
      "choices": {
        "A": "Edges are sharper",
        "B": "Sum of kernel elements equals 1, preserving average intensity",
        "C": "Kernel becomes integer-valued",
        "D": "Noise is removed"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Why can filtering with uint8 cause unwanted effects?",
      "choices": {
        "A": "It increases spatial resolution",
        "B": "Clipping and quantisation may cause loss of precision",
        "C": "It makes convolution non-linear",
        "D": "It changes kernel size"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is the identity filter?",
      "choices": {
        "A": "A kernel of zeros",
        "B": "A kernel with a 1 in the centre and zeros elsewhere",
        "C": "A Gaussian filter",
        "D": "A sharpening filter"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is the moving average (box) filter?",
      "choices": {
        "A": "Median of neighbourhood",
        "B": "Kernel of ones divided by number of elements, computing average",
        "C": "Gaussian kernel",
        "D": "Edge detector"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What characterises the Gaussian filter?",
      "choices": {
        "A": "Equal weights",
        "B": "Weights sampled from a 2D Gaussian distribution with highest weight at centre",
        "C": "Binary mask",
        "D": "Median selection"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What does σ control in Gaussian filtering?",
      "choices": {
        "A": "Image resolution",
        "B": "Amount of blurring",
        "C": "Edge thickness",
        "D": "Kernel symmetry"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is the purpose of a sharpening filter?",
      "choices": {
        "A": "Blur the image",
        "B": "Make edges more pronounced by adding image details",
        "C": "Reduce bit depth",
        "D": "Remove histogram peaks"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What defines non-linear filtering?",
      "choices": {
        "A": "Representable via convolution",
        "B": "Cannot be represented as convolution",
        "C": "Uses Gaussian weights",
        "D": "Uses normalised kernel"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is the median filter?",
      "choices": {
        "A": "Computes weighted average",
        "B": "Output pixel equals median of neighbourhood values",
        "C": "Computes second derivative",
        "D": "Normalises intensity"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Why is the median filter effective against salt & pepper noise?",
      "choices": {
        "A": "It averages out extremes",
        "B": "It ignores extreme values when selecting the median",
        "C": "It sharpens edges",
        "D": "It uses Gaussian weights"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What is edge detection?",
      "choices": {
        "A": "Detecting colour changes only",
        "B": "Identifying locations of significant intensity change",
        "C": "Computing histogram",
        "D": "Applying smoothing"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Gradient-based edge detection is based on:",
      "choices": {
        "A": "Second derivative",
        "B": "First derivative of intensity",
        "C": "Median filter",
        "D": "Kernel normalisation"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Laplacian-based edge detection uses:",
      "choices": {
        "A": "First derivative",
        "B": "Second derivative",
        "C": "Gaussian smoothing only",
        "D": "Median filter"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "What does non-maximum suppression do?",
      "choices": {
        "A": "Blurs edges",
        "B": "Keeps only local maxima along gradient direction",
        "C": "Normalises intensities",
        "D": "Applies Gaussian blur"
      },
      "answer": "B"
    },
{
  "section": "image-filtering-and-edge-detection",
  "question": "What is the primary goal of image filtering according to the lecture?",
  "choices": {
    "A": "To increase pixel resolution",
    "B": "To remove unwanted components or enhance desired ones",
    "C": "To change colour space",
    "D": "To convert analogue signals into digital"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Filtering can be applied to colour images by:",
  "choices": {
    "A": "Filtering only the red channel",
    "B": "Filtering each channel separately or filtering the V channel in HSV",
    "C": "Converting to CMYK first",
    "D": "Removing two channels"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "In window-based image filtering, the output pixel J(x,y) depends on:",
  "choices": {
    "A": "Only the pixel I(x,y)",
    "B": "A neighbourhood of pixels around (x,y)",
    "C": "The histogram only",
    "D": "The entire image"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "What defines the neighbourhood in window-based filtering?",
  "choices": {
    "A": "Histogram bins",
    "B": "Kernel normalisation",
    "C": "A window of size N × N",
    "D": "Bit depth"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Linear filtering requires that:",
  "choices": {
    "A": "The output is the median of neighbours",
    "B": "Pixel values are a linear combination of neighbours with fixed weights",
    "C": "The kernel changes per pixel",
    "D": "The image is grayscale"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The matrix of weights used in linear filtering is called:",
  "choices": {
    "A": "Footprint",
    "B": "Histogram",
    "C": "Kernel",
    "D": "CDF"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The mathematical expression J(x,y) = Σ K(n,m) I(x+n, y+m) describes:",
  "choices": {
    "A": "Median filtering",
    "B": "Convolution (linear filtering)",
    "C": "Histogram equalisation",
    "D": "Gamma correction"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Technically, the formula shown in the slides corresponds to:",
  "choices": {
    "A": "True convolution with flipped kernel",
    "B": "Cross-correlation",
    "C": "Fourier transform",
    "D": "Median filtering"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Which property allows decomposition of a 2D kernel into two 1D kernels?",
  "choices": {
    "A": "Quantisation",
    "B": "Separability due to convolution properties",
    "C": "Median invariance",
    "D": "Histogram symmetry"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Which is NOT a property of convolution?",
  "choices": {
    "A": "Commutative",
    "B": "Distributive",
    "C": "Associative",
    "D": "Non-linear"
  },
  "answer": "D"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Zero padding handles edges by:",
  "choices": {
    "A": "Replicating nearest border pixels",
    "B": "Wrapping pixels from opposite side",
    "C": "Extending image with zeros",
    "D": "Reducing image size"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Nearest neighbour edge handling:",
  "choices": {
    "A": "Uses values from opposite border",
    "B": "Replicates nearest border pixels",
    "C": "Fills with zeros",
    "D": "Removes borders"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Wrapping edge handling:",
  "choices": {
    "A": "Uses zero values",
    "B": "Copies nearest pixel",
    "C": "Extends using values from opposite side",
    "D": "Blurs borders"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "A normalised kernel ensures that:",
  "choices": {
    "A": "Edges are sharper",
    "B": "The average intensity remains unchanged",
    "C": "Kernel values are integers",
    "D": "The image becomes binary"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Why is it advisable to convert uint8 images to float before filtering?",
  "choices": {
    "A": "To increase resolution",
    "B": "To avoid clipping/quantisation loss of precision",
    "C": "To reduce memory",
    "D": "To change colour space"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The moving average (box) filter output is:",
  "choices": {
    "A": "The maximum of window",
    "B": "The minimum of window",
    "C": "The average of pixel values in window",
    "D": "The central pixel"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Increasing the box filter size primarily:",
  "choices": {
    "A": "Reduces smoothing",
    "B": "Increases blurring and removes more detail",
    "C": "Increases edge sharpness",
    "D": "Changes dynamic range"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "In a Gaussian filter, the central pixel weight is:",
  "choices": {
    "A": "Smallest",
    "B": "Equal to others",
    "C": "Largest",
    "D": "Zero"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The parameter σ in Gaussian filtering controls:",
  "choices": {
    "A": "Kernel size only",
    "B": "Amount of blurring",
    "C": "Edge direction",
    "D": "Bit depth"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Sharpening filters enhance:",
  "choices": {
    "A": "Low-frequency components",
    "B": "Image details (differences from local average)",
    "C": "Noise only",
    "D": "Histogram bins"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The median filter is considered non-linear because:",
  "choices": {
    "A": "It uses convolution",
    "B": "The output is not a linear combination of inputs",
    "C": "It uses Gaussian weights",
    "D": "It preserves intensity sum"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Median filtering is particularly effective against:",
  "choices": {
    "A": "Gaussian blur",
    "B": "Salt & pepper noise",
    "C": "Lens distortion",
    "D": "Low resolution"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "The median filter output pixel equals:",
  "choices": {
    "A": "Mean of window",
    "B": "Central pixel",
    "C": "Median of values in window",
    "D": "Maximum value"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Which filter preserves edges better while removing impulse noise?",
  "choices": {
    "A": "Box filter",
    "B": "Gaussian filter",
    "C": "Median filter",
    "D": "Identity filter"
  },
  "answer": "C"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Edge detection aims to identify:",
  "choices": {
    "A": "Uniform intensity regions",
    "B": "Locations of significant intensity change",
    "C": "Histogram peaks",
    "D": "Colour models"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Gradient-based edge detection relies on:",
  "choices": {
    "A": "Second derivatives",
    "B": "First derivatives",
    "C": "Median values",
    "D": "Histogram equalisation"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Laplacian-based edge detection relies on:",
  "choices": {
    "A": "First derivative",
    "B": "Second derivative",
    "C": "Histogram",
    "D": "Gaussian blur only"
  },
  "answer": "B"
},
{
  "section": "image-filtering-and-edge-detection",
  "question": "Non-maximum suppression is used to:",
  "choices": {
    "A": "Blur edges",
    "B": "Thin edges by keeping local maxima in gradient direction",
    "C": "Normalise kernel",
    "D": "Extend borders"
  },
  "answer": "B"
},
  {
    "section": "image-filtering-and-edge-detection",
    "question": "In window-based image filtering, the output pixel J(x,y) is computed from:",
    "choices": {
      "A": "Only the single input pixel I(x,y)",
      "B": "A neighbourhood of input pixels around (x,y) defined by a window",
      "C": "Only histogram statistics",
      "D": "Only edge pixels"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Convolution of an image I with a kernel K is a linear operator, which implies it is:",
    "choices": {
      "A": "Non-commutative and non-associative",
      "B": "Commutative, distributive, and associative (under the usual assumptions)",
      "C": "Only defined for 1D signals",
      "D": "Only defined for binary images"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A normalised smoothing kernel is typically designed so that:",
    "choices": {
      "A": "The sum of kernel coefficients is 0",
      "B": "The sum of kernel coefficients is 1 to preserve average intensity",
      "C": "The centre coefficient is 0",
      "D": "All coefficients are negative"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which boundary-handling strategy extends the image using 0s?",
    "choices": {
      "A": "Nearest neighbour padding",
      "B": "Zero padding",
      "C": "Wrapping",
      "D": "Mirroring only"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which boundary-handling strategy extends the image by replicating the nearest border pixel values?",
    "choices": {
      "A": "Zero padding",
      "B": "Nearest neighbour padding (replication)",
      "C": "Wrapping",
      "D": "Random padding"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which boundary-handling strategy extends the image using values from the opposite side?",
    "choices": {
      "A": "Zero padding",
      "B": "Nearest neighbour padding",
      "C": "Wrapping",
      "D": "Gaussian padding"
    },
    "answer": "C"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "The identity filter kernel has the effect that the output image is:",
    "choices": {
      "A": "Blurred",
      "B": "Identical to the input",
      "C": "Edge-only",
      "D": "Inverted"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A moving average (box) filter is primarily used for:",
    "choices": {
      "A": "Sharpening edges",
      "B": "Smoothing / blurring (noise reduction)",
      "C": "Detecting corners",
      "D": "Estimating homographies"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A Gaussian filter differs from a box filter mainly because:",
    "choices": {
      "A": "It is non-linear",
      "B": "It gives higher weight to pixels near the centre of the window",
      "C": "It cannot blur images",
      "D": "It uses only negative weights"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "In a Gaussian filter, increasing σ generally:",
    "choices": {
      "A": "Produces stronger smoothing and emphasises larger-scale structures",
      "B": "Detects finer details and more noise",
      "C": "Makes the operation non-linear",
      "D": "Removes the need for edge handling"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A sharpening filter can be interpreted as:",
    "choices": {
      "A": "Adding high-frequency 'details' (difference from local average) to the image",
      "B": "Replacing each pixel with the median",
      "C": "Making the histogram uniform",
      "D": "Removing all edges"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Why is the median filter considered non-linear?",
    "choices": {
      "A": "Because it uses negative weights",
      "B": "Because sorting and selecting the median is not a linear operation",
      "C": "Because it requires a Fourier transform",
      "D": "Because it always changes image size"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Median filtering is especially effective for removing:",
    "choices": {
      "A": "Gaussian noise",
      "B": "Salt-and-pepper noise",
      "C": "Perspective distortion",
      "D": "Colour misregistration"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "The image gradient ∇I at a pixel is a vector consisting of:",
    "choices": {
      "A": "Second derivatives only",
      "B": "First derivatives in x and y (Ix, Iy)",
      "C": "Histogram mean and variance",
      "D": "RGB channel values"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "The gradient magnitude is typically computed as:",
    "choices": {
      "A": "Ix + Iy",
      "B": "sqrt(Ix^2 + Iy^2)",
      "C": "Ix^2 - Iy^2",
      "D": "arctan2(Iy, Ix)"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "The gradient direction θ(x,y) is typically computed using:",
    "choices": {
      "A": "arctan2(Iy, Ix)",
      "B": "sqrt(Ix^2 + Iy^2)",
      "C": "Ix * Iy",
      "D": "det(M) - k(trace(M))^2"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Why are derivative-based edge detectors sensitive to noise?",
    "choices": {
      "A": "Derivatives amplify high-frequency components, including noise",
      "B": "Noise cancels out under differentiation",
      "C": "Derivatives average out noise completely",
      "D": "Noise affects only colour images"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Prewitt and Sobel filters are commonly applied after smoothing because:",
    "choices": {
      "A": "Smoothing reduces noise before computing derivatives",
      "B": "Smoothing increases edge strength by definition",
      "C": "Smoothing converts RGB to grayscale",
      "D": "Smoothing increases bit depth"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Laplacian-based edge detection often looks for edges by finding:",
    "choices": {
      "A": "Maxima of gradient magnitude",
      "B": "Zero-crossings of the second derivative (Laplacian)",
      "C": "Histogram peaks",
      "D": "Local maxima of Harris response"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A low threshold for gradient-magnitude edge detection tends to:",
    "choices": {
      "A": "Miss subtle edges",
      "B": "Detect more noise and irrelevant edges",
      "C": "Guarantee single-pixel edges",
      "D": "Improve localisation always"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A high threshold for gradient-magnitude edge detection tends to:",
    "choices": {
      "A": "Reduce noise but may miss subtle edges or fragment edges",
      "B": "Increase noise detection",
      "C": "Increase image resolution",
      "D": "Create more edges than a low threshold"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which sequence matches the main steps of the Canny edge detector?",
    "choices": {
      "A": "Histogram equalisation → median filter → thresholding → dilation",
      "B": "Gaussian smoothing → gradient magnitude/direction → non-maximum suppression → hysteresis thresholding",
      "C": "SIFT detection → descriptor → matching → affine estimation",
      "D": "Box filter → Laplacian → PCA → clustering"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Non-maximum suppression (NMS) in Canny is used to:",
    "choices": {
      "A": "Make edges thicker",
      "B": "Thin edges by keeping only local maxima along the gradient direction",
      "C": "Quantise intensity values",
      "D": "Compute colour histograms"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "In hysteresis thresholding (T_high, T_low), a weak pixel between thresholds is accepted as an edge if:",
    "choices": {
      "A": "It is isolated",
      "B": "It is connected to a strong edge pixel",
      "C": "Its intensity is 0",
      "D": "It has zero gradient"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Increasing the Gaussian σ in Canny tends to:",
    "choices": {
      "A": "Detect finer details and more noise",
      "B": "Suppress fine details and detect larger-scale edges",
      "C": "Remove the need for hysteresis",
      "D": "Make gradient direction undefined"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "In the slides, a suggested Python function for linear filtering (convolution) is:",
    "choices": {
      "A": "skimage.feature.canny",
      "B": "scipy.ndimage.convolve",
      "C": "numpy.linalg.svd",
      "D": "skimage.transform.warp"
    },
    "answer": "B"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "When filtering uint8 images, why is converting to float often advisable?",
    "choices": {
      "A": "To avoid clipping/quantisation and preserve precision during computation",
      "B": "To make the kernel separable",
      "C": "To increase pixel density",
      "D": "To ensure edges disappear"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  {
    "section": "image-filtering-and-edge-detection",
    "question": "In Canny, why is 'single response' a desirable property?",
    "choices": {
      "A": "So each true edge ideally produces one thin response rather than multiple parallel responses",
      "B": "So each edge is detected many times",
      "C": "So the image becomes binary everywhere",
      "D": "So gradients are not computed"
    },
    "answer": "A"
  },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In general, filtering allows to:",
      "choices": {
        "A": "Increase spatial resolution",
        "B": "Remove unwanted components from a given signal or enhance some desired ones",
        "C": "Change image format",
        "D": "Estimate geometric transformations"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In window-based image filtering, the value of each pixel (x,y) in the output image J is:",
      "choices": {
        "A": "Independent of neighbouring pixels",
        "B": "A function of the pixel values of the input image I in a neighbourhood of (x,y)",
        "C": "Only determined by histogram values",
        "D": "Randomly assigned"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "If the pixel values in the output image are a linear combination of pixel values in a neighbourhood and the weights are fixed for all positions, then we are performing:",
      "choices": {
        "A": "Non-linear filtering",
        "B": "Gradient thresholding",
        "C": "Linear filtering",
        "D": "Hysteresis"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The act of moving a linear kernel on an image is called:",
      "choices": {
        "A": "Quantisation",
        "B": "Convolution",
        "C": "Normalisation",
        "D": "Interpolation"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Filtering with a normalised kernel ensures that:",
      "choices": {
        "A": "The image becomes sharper",
        "B": "The average intensity for the whole image remains unchanged",
        "C": "The datatype becomes float",
        "D": "Edges are removed"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "If datatype is uint8 during filtering, values are forced to be integers in the range:",
      "choices": {
        "A": "[-1,1]",
        "B": "[0,1]",
        "C": "[0,255]",
        "D": "[-255,255]"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Why is it advisable to convert to float before filtering?",
      "choices": {
        "A": "To change spatial resolution",
        "B": "To avoid clipping/quantisation and unwanted loss of precision",
        "C": "To apply HSV conversion",
        "D": "To increase kernel size"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The kernel of an identity filter is:",
      "choices": {
        "A": "All ones",
        "B": "All zeros",
        "C": "A set of 0s with a 1 in the centre",
        "D": "A Gaussian distribution"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The output of an identity filter will be:",
      "choices": {
        "A": "Blurred",
        "B": "Equal to the input pixel value",
        "C": "Binary",
        "D": "Sharpened"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The output pixel value of a moving average filter is:",
      "choices": {
        "A": "The maximum of the window",
        "B": "The median of the window",
        "C": "The average of the pixel values in the window",
        "D": "The derivative of the window"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Increasing the footprint size in a moving average filter results in:",
      "choices": {
        "A": "Less smoothing",
        "B": "More smoothing and blurring",
        "C": "Edge detection",
        "D": "Higher contrast"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In a Gaussian filter, the central pixel has:",
      "choices": {
        "A": "The lowest weight",
        "B": "The highest weight",
        "C": "Zero weight",
        "D": "Negative weight"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The standard deviation σ in a Gaussian filter controls:",
      "choices": {
        "A": "The image resolution",
        "B": "The amount of blurring",
        "C": "The edge threshold",
        "D": "The datatype"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "A sharpening filter makes:",
      "choices": {
        "A": "Noise disappear",
        "B": "Edges more pronounced",
        "C": "The image binary",
        "D": "The image grayscale"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The median filter output pixel will be:",
      "choices": {
        "A": "The average of pixel values",
        "B": "The median of the pixel values in the window",
        "C": "The derivative value",
        "D": "The gradient magnitude"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The median filter is especially useful in the presence of:",
      "choices": {
        "A": "Gaussian blur",
        "B": "Salt & pepper noise",
        "C": "Lens distortion",
        "D": "Compression artefacts"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Edge detection consists in locating:",
      "choices": {
        "A": "Regions of constant intensity",
        "B": "Strong discontinuities in pixel intensity or colour",
        "C": "Gaussian kernels",
        "D": "Histogram peaks"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The image gradient ∇I is a vector consisting of:",
      "choices": {
        "A": "Two second derivatives",
        "B": "The Laplacian only",
        "C": "The two first derivatives computed at each point",
        "D": "Median responses"
      },
      "answer": "C"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The magnitude of the gradient is computed as:",
      "choices": {
        "A": "Ix + Iy",
        "B": "sqrt(Ix^2 + Iy^2)",
        "C": "Ix - Iy",
        "D": "Ix * Iy"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Non-maximum suppression (NMS) is:",
      "choices": {
        "A": "A smoothing technique",
        "B": "An edge thinning technique",
        "C": "A histogram equalisation method",
        "D": "A quantisation method"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In hysteresis thresholding, if gradient magnitude is larger than Thigh, the pixel is:",
      "choices": {
        "A": "Rejected",
        "B": "Accepted as an edge",
        "C": "Smoothed",
        "D": "Set to median"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In hysteresis thresholding, if gradient magnitude is lower than Tlow, the pixel is:",
      "choices": {
        "A": "Accepted",
        "B": "Rejected",
        "C": "Smoothed",
        "D": "Converted to float"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "In the Canny edge detector, the first step is:",
      "choices": {
        "A": "Hysteresis thresholding",
        "B": "Gaussian filtering to suppress noise",
        "C": "Non-maximum suppression",
        "D": "Binary thresholding"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "High σ in the Canny detector will:",
      "choices": {
        "A": "Detect fine details",
        "B": "Detect large scale structures",
        "C": "Increase noise sensitivity",
        "D": "Disable NMS"
      },
      "answer": "B"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "Low σ in the Canny detector will:",
      "choices": {
        "A": "Detect fine details",
        "B": "Detect only large objects",
        "C": "Remove edges",
        "D": "Skip gradient computation"
      },
      "answer": "A"
    },
    {
      "section": "image-filtering-and-edge-detection",
      "question": "The function recommended for convolution in Python in the slides is:",
      "choices": {
        "A": "filters.median",
        "B": "scipy.ndimage.convolve",
        "C": "skimage.feature.canny",
        "D": "np.gradient"
      },
      "answer": "B"
    },
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which statement about gradient magnitude is correct?",
    "choices": {
      "A": "It highlights locations of strong intensity change, often corresponding to edges",
      "B": "It is always 0 in images",
      "C": "It equals the histogram CDF",
      "D": "It is invariant to σ in all cases"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  
  
  
  
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which filter is best matched to the goal 'reduce salt-and-pepper noise'?",
    "choices": {
      "A": "Median filter",
      "B": "Sharpening filter",
      "C": "Identity filter",
      "D": "Edge detector"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  
  
  
  
  
  {
    "section": "image-filtering-and-edge-detection",
    "question": "A kernel with coefficients that sum to 1 is useful because it:",
    "choices": {
      "A": "Preserves the average intensity (no overall brightness shift)",
      "B": "Always sharpens the image",
      "C": "Always produces a binary output",
      "D": "Guarantees no noise"
    },
    "answer": "A"
  },
  {
    "section": "image-filtering-and-edge-detection",
    "question": "Which operation is NOT linear and cannot be represented as convolution?",
    "choices": {
      "A": "Gaussian smoothing",
      "B": "Box filtering",
      "C": "Median filtering",
      "D": "Identity filtering"
    },
    "answer": "C"
  },
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In image matching/alignment/registration, the main goal is typically to:",
    "choices": {
      "A": "Increase bit depth",
      "B": "Estimate a transformation that aligns one image to another",
      "C": "Equalise histograms globally",
      "D": "Detect edges only"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "A major advantage of matching by interest points is that it:",
    "choices": {
      "A": "Uses all pixels for maximum accuracy",
      "B": "Is efficient because it matches a sparse set of distinctive points",
      "C": "Avoids the need for descriptors",
      "D": "Works only for perfectly aligned images"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "According to the slides, a good interest point should be:",
    "choices": {
      "A": "Located in flat regions to reduce noise",
      "B": "Distinctive and repeatable under perturbations (e.g., illumination changes)",
      "C": "Any pixel with intensity 0",
      "D": "Always on an edge, never a corner"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Why are corners often preferred as interest points over edge points?",
    "choices": {
      "A": "Corners change only in one direction",
      "B": "Corners have intensity changes in two directions, making them more distinctive",
      "C": "Edges are invariant to rotation",
      "D": "Corners require no derivatives"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the Harris corner detector, the matrix M is computed from:",
    "choices": {
      "A": "Colour channel means",
      "B": "Local image derivatives (Ix, Iy) within a window",
      "C": "Histogram CDF values",
      "D": "DoG pyramid values only"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "The Harris response is commonly defined as R = det(M) − k(trace(M))². A large positive R usually indicates:",
    "choices": {
      "A": "A flat region",
      "B": "A likely corner",
      "C": "A uniform histogram",
      "D": "A perfect match"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "The Harris detector is rotation-invariant because:",
    "choices": {
      "A": "It uses RGB values",
      "B": "The eigenvalues of M do not change under rotation",
      "C": "It ignores gradients",
      "D": "It uses only second derivatives"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "The basic Harris detector is not scale-invariant mainly because:",
    "choices": {
      "A": "It cannot compute eigenvalues",
      "B": "A fixed window size responds differently when structures appear at different scales",
      "C": "It requires colour images",
      "D": "It cannot be thresholded"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Scale-space representation is typically created by:",
    "choices": {
      "A": "Applying median filters with random window sizes",
      "B": "Smoothing with Gaussians of increasing σ (and/or resizing) to represent different scales",
      "C": "Converting RGB to CMYK",
      "D": "Thresholding gradients"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In SIFT, candidate keypoints are detected as extrema of:",
    "choices": {
      "A": "The Laplacian ΔI directly",
      "B": "Difference-of-Gaussians (DoG) images across space and scale",
      "C": "Histogram bins across intensity",
      "D": "Median-filtered images only"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In SIFT, rotation invariance is achieved mainly by:",
    "choices": {
      "A": "Normalising intensities to [0,1]",
      "B": "Assigning a dominant orientation to each keypoint and rotating gradients accordingly",
      "C": "Using a larger σ only",
      "D": "Using a 64-D descriptor"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the SIFT descriptor described, the 128-D descriptor comes from:",
    "choices": {
      "A": "16 subregions × 8 orientation bins",
      "B": "8 subregions × 16 bins",
      "C": "4 subregions × 32 bins",
      "D": "128 subregions × 1 bin"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Why is gradient orientation often more robust than gradient magnitude to absolute intensity changes?",
    "choices": {
      "A": "Orientation depends on relative changes, not absolute scaling of intensities",
      "B": "Orientation is always zero",
      "C": "Magnitude is invariant to scaling",
      "D": "Orientation equals the histogram CDF"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Why can a histogram of values in a patch be more robust to rotation than flattened patch values?",
    "choices": {
      "A": "Rotation preserves histogram bin counts while changing spatial arrangement",
      "B": "Rotation always changes intensity values",
      "C": "Histograms store coordinates explicitly",
      "D": "Flattened patches are rotation-invariant"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Compared to SIFT, SURF was proposed mainly to:",
    "choices": {
      "A": "Make descriptors higher-dimensional",
      "B": "Speed up detection/description using approximations and often a 64-D descriptor",
      "C": "Remove the need for gradients",
      "D": "Work only on binary images"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In SURF, Haar wavelets are used to approximate:",
    "choices": {
      "A": "Histogram equalisation",
      "B": "Local gradient-like responses efficiently",
      "C": "Colour demosaicing",
      "D": "Perspective projection"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "A version of SURF that omits orientation assignment (U-SURF) is not:",
    "choices": {
      "A": "Scale-invariant",
      "B": "Rotation-invariant",
      "C": "A local descriptor",
      "D": "Fast"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "A common reason to reject 'single pixel intensity' as a feature descriptor is that it is:",
    "choices": {
      "A": "Too high-dimensional",
      "B": "Not discriminative and sensitive to illumination changes",
      "C": "Invariant to all perturbations",
      "D": "Always rotation-invariant"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Flattened raw patch intensities as a descriptor are particularly weak because they are:",
    "choices": {
      "A": "Invariant to rotation and intensity scaling",
      "B": "Sensitive to rotation and global intensity changes",
      "C": "Independent of scale-space",
      "D": "Always unique for every point"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Matching by edges can be useful because:",
    "choices": {
      "A": "Edges provide a sparse representation that can reduce data used for alignment",
      "B": "Edges guarantee perfect correspondences",
      "C": "Edges remove the need for a transformation model",
      "D": "Edges are always scale-invariant"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Which computer vision task is best described as: Identify the class of the object shown in the image?",
    "choices": {
      "A": "Image classification",
      "B": "Object detection",
      "C": "Semantic segmentation",
      "D": "Instance segmentation"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Which task extends image classification by also regressing a bounding box for a single object?",
    "choices": {
      "A": "Image classification",
      "B": "Image classification & localization",
      "C": "Semantic segmentation",
      "D": "Image matching"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Which computer vision task is best described as: Identify classes and bounding boxes for multiple objects?",
    "choices": {
      "A": "Image classification",
      "B": "Object detection",
      "C": "Semantic segmentation",
      "D": "Instance segmentation"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Which computer vision task is best described as: Assign a class label to each pixel?",
    "choices": {
      "A": "Image classification",
      "B": "Object detection",
      "C": "Semantic segmentation",
      "D": "Instance segmentation"
    },
    "answer": "C"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Which computer vision task is best described as: Assign a label to each pixel and distinguish different instances?",
    "choices": {
      "A": "Image classification",
      "B": "Object detection",
      "C": "Semantic segmentation",
      "D": "Instance segmentation"
    },
    "answer": "D"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "To estimate a 2D affine transform from point correspondences, the minimum number of point pairs required is:",
    "choices": {
      "A": "2",
      "B": "3",
      "C": "4",
      "D": "6"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the slides, which method is used to estimate an affine transform from corresponding points in skimage?",
    "choices": {
      "A": "tform.estimate",
      "B": "exposure.equalize_hist",
      "C": "feature.canny",
      "D": "filters.median"
    },
    "answer": "A"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "When using transform.warp(img, tform), what does tform represent?",
    "choices": {
      "A": "A histogram binning rule",
      "B": "A geometric mapping model (e.g., affine) used to map coordinates",
      "C": "A colour conversion matrix",
      "D": "A JPEG quantisation table"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the Harris framework, a flat region corresponds to eigenvalues (λ1, λ2) that are:",
    "choices": {
      "A": "Both large",
      "B": "One large, one small",
      "C": "Both small",
      "D": "Both negative"
    },
    "answer": "C"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the Harris framework, an edge corresponds to eigenvalues (λ1, λ2) that are:",
    "choices": {
      "A": "Both large",
      "B": "One large, one small",
      "C": "Both small",
      "D": "Equal and large"
    },
    "answer": "B"
  },
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the Harris framework, a corner corresponds to eigenvalues (λ1, λ2) that are:",
    "choices": {
      "A": "Both large",
      "B": "One large, one small",
      "C": "Both small",
      "D": "Both zero"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In the SIFT descriptor, each subregion histogram bin vote is weighted by:",
    "choices": {
      "A": "Gradient magnitude",
      "B": "Pixel intensity only",
      "C": "PPI",
      "D": "JPEG quality factor"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "A descriptor based on a histogram of gradient orientations helps mainly because it can combine:",
    "choices": {
      "A": "Robustness to intensity scaling (orientation) and robustness to rotation (histogram)",
      "B": "Compression and demosaicing",
      "C": "Bit depth and PPI",
      "D": "HSV and CMYK"
    },
    "answer": "A"
  },
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "During SIFT keypoint refinement (as mentioned), candidates lying on edges are often discarded because:",
    "choices": {
      "A": "They are ambiguous: they are invariant to translations parallel to the edge direction",
      "B": "They are always too dark",
      "C": "They violate commutativity",
      "D": "They increase JPEG artefacts"
    },
    "answer": "A"
  },
  
  
  
  
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "In SIFT, the orientation histogram per subregion typically has:",
    "choices": {
      "A": "4 bins",
      "B": "8 bins (45° each)",
      "C": "16 bins (22.5° each)",
      "D": "128 bins"
    },
    "answer": "B"
  },
  
  {
    "section": "image-matching-interest-point-detection-and-feature-descriptors",
    "question": "Why do we need a feature descriptor after detecting keypoints?",
    "choices": {
      "A": "To describe each keypoint so similarity can be evaluated across images",
      "B": "To increase image resolution",
      "C": "To remove noise by smoothing",
      "D": "To compute the histogram CDF"
    },
    "answer": "A"
  },
  
  
  
  
    {
      "section": "digital-images-and-image-processing",
      "question": "What does the RGB colour model fundamentally represent?",
      "choices": {
        "A": "Amounts of red, green, and blue light emitted or combined",
        "B": "Amounts of cyan, magenta, and yellow ink",
        "C": "Temperature and brightness values",
        "D": "Spatial resolution in three dimensions"
      },
      "answer": "A"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "The RGB colour space is commonly represented geometrically as:",
      "choices": {
        "A": "A cylinder",
        "B": "A triangle",
        "C": "A cube",
        "D": "A sphere"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In the RGB cube representation, what does the origin (0,0,0) correspond to?",
      "choices": {
        "A": "White",
        "B": "Black",
        "C": "Pure red",
        "D": "Maximum brightness"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In the RGB cube, the point (255,255,255) represents:",
      "choices": {
        "A": "Black",
        "B": "Pure blue",
        "C": "White",
        "D": "Yellow"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Why is RGB considered an additive colour model?",
      "choices": {
        "A": "Because ink absorbs light",
        "B": "Because colours are formed by adding light intensities together",
        "C": "Because it subtracts from white",
        "D": "Because it reduces dynamic range"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What does the CMYK colour model represent?",
      "choices": {
        "A": "Emission of coloured light",
        "B": "Subtractive mixing of inks in printing",
        "C": "Hue, saturation, and value",
        "D": "Opponent perceptual axes"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Why is black (K) included in CMYK printing?",
      "choices": {
        "A": "To reduce file size",
        "B": "To achieve deeper blacks and improve print quality",
        "C": "To increase brightness",
        "D": "To simplify RGB conversion"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "HSV colour space is often represented geometrically as:",
      "choices": {
        "A": "A cube",
        "B": "A cylinder",
        "C": "A flat plane",
        "D": "A pyramid"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In HSV colour space, what does Hue represent?",
      "choices": {
        "A": "Brightness level",
        "B": "Dominant wavelength (discernible colour)",
        "C": "Number of pixels",
        "D": "Ink absorption rate"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In HSV colour space, what does Saturation represent?",
      "choices": {
        "A": "Intensity precision",
        "B": "Vividness or purity of the colour",
        "C": "Spatial resolution",
        "D": "Number of channels"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In HSV colour space, what does Value represent?",
      "choices": {
        "A": "Brightness of the colour",
        "B": "Number of bits per pixel",
        "C": "Dynamic range only",
        "D": "Distance in LAB space"
      },
      "answer": "A"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Which colour model attempts perceptual uniformity?",
      "choices": {
        "A": "RGB",
        "B": "CMYK",
        "C": "HSV",
        "D": "CIE LAB"
      },
      "answer": "D"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In LAB colour space, the 'a' axis represents:",
      "choices": {
        "A": "Brightness only",
        "B": "Blue versus yellow",
        "C": "Red versus green",
        "D": "Hue angle"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In LAB colour space, the 'b' axis represents:",
      "choices": {
        "A": "Red versus green",
        "B": "Blue versus yellow",
        "C": "Brightness only",
        "D": "Spatial depth"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Why is LAB considered perceptually uniform?",
      "choices": {
        "A": "Because it uses 24 bits",
        "B": "Because equal distances in LAB space approximate equal perceived colour differences",
        "C": "Because it is cylindrical",
        "D": "Because it uses additive mixing"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "How is light captured in digital cameras separated into RGB values?",
      "choices": {
        "A": "Through histogram equalisation",
        "B": "Using a colour filter array (CFA)",
        "C": "Using JPEG compression",
        "D": "Using gamma correction"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What is a colour filter array (CFA)?",
      "choices": {
        "A": "A compression algorithm",
        "B": "A grid of coloured filters placed over the sensor",
        "C": "A histogram transformation",
        "D": "A type of colour space"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What is the Bayer pattern?",
      "choices": {
        "A": "A lossy compression method",
        "B": "The most common CFA design based on a GRBG arrangement",
        "C": "A colour cube representation",
        "D": "A histogram equalisation technique"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Why does the Bayer pattern contain twice as many green filters as red or blue?",
      "choices": {
        "A": "Green sensors are cheaper",
        "B": "The human eye is more sensitive to green wavelengths",
        "C": "Green increases resolution",
        "D": "It reduces file size"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What does the demosaicing algorithm do?",
      "choices": {
        "A": "Reduces spatial resolution",
        "B": "Interpolates missing colour values so each pixel has R, G, and B values",
        "C": "Compresses the image",
        "D": "Converts RGB to LAB"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What does the demosaicing algorithm ensure?",
      "choices": {
        "A": "Bit depth increases",
        "B": "Final pixel resolution is preserved",
        "C": "Dynamic range increases",
        "D": "Colour channels are reduced"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "A RAW image file primarily contains:",
      "choices": {
        "A": "Fully demosaiced RGB pixels",
        "B": "Interpolated colour data only",
        "C": "Sensor data before demosaicing",
        "D": "Compressed JPEG pixels"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What does an RGBD image contain in addition to RGB values?",
      "choices": {
        "A": "Temperature data",
        "B": "Depth information for each pixel",
        "C": "Two additional colour channels",
        "D": "Compression metadata"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In an RGBD image defined as I(x,y): Z² → Z⁴, what does the fourth value represent?",
      "choices": {
        "A": "Hue",
        "B": "Time",
        "C": "Depth from the camera",
        "D": "Gamma correction"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What does the depth channel (D) in an RGBD image measure?",
      "choices": {
        "A": "Brightness of the pixel",
        "B": "Distance between the object and the camera",
        "C": "Colour saturation",
        "D": "Pixel density"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Which device is typically required to capture RGBD data?",
      "choices": {
        "A": "Standard webcam only",
        "B": "Camera with an additional depth sensor",
        "C": "Laser printer",
        "D": "JPEG encoder"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "What are volumetric images?",
      "choices": {
        "A": "Images stored as JPEG files",
        "B": "2D grayscale images",
        "C": "3D image data stored as voxels",
        "D": "Colour-only images"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In a volumetric image defined as I(x,y,z): Z³ → Z, what does (x,y,z) represent?",
      "choices": {
        "A": "Three colour channels",
        "B": "Spatial coordinates in 3D space",
        "C": "Time and colour",
        "D": "Resolution and depth"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "The smallest unit in a volumetric image is called:",
      "choices": {
        "A": "Pixel",
        "B": "Byte",
        "C": "Voxel",
        "D": "Bit"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "A CT scan is an example of:",
      "choices": {
        "A": "RGB image",
        "B": "Colour video",
        "C": "Volumetric image",
        "D": "Binary image"
      },
      "answer": "C"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "How is a colour video mathematically represented?",
      "choices": {
        "A": "I(x,y): Z² → Z",
        "B": "I(x,y,t): Z³ → Z³",
        "C": "I(x,y,z): Z³ → Z",
        "D": "I(x): Z → Z"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "In a colour video I(x,y,t), what does 't' represent?",
      "choices": {
        "A": "Temperature",
        "B": "Time",
        "C": "Texture",
        "D": "Threshold"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Why is a colour video considered a 3D signal?",
      "choices": {
        "A": "Because it contains RGB channels",
        "B": "Because it includes spatial dimensions plus time",
        "C": "Because it has depth information",
        "D": "Because it is volumetric"
      },
      "answer": "B"
    },
    {
      "section": "digital-images-and-image-processing",
      "question": "Which of the following distinguishes a volumetric image from a colour video?",
      "choices": {
        "A": "Volumetric images include time",
        "B": "Colour videos include spatial depth (z)",
        "C": "Volumetric images use spatial depth (z) while videos use time (t)",
        "D": "They are identical"
      },
      "answer": "C"
    },
      {
        "section": "digital-images-and-image-processing",
        "question": "What type of object is returned by skimage.io.imread?",
        "choices": {
          "A": "A Python list",
          "B": "A NumPy ndarray",
          "C": "A PIL Image object",
          "D": "A dictionary"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "If img.shape returns (777, 1024, 3), what does 3 represent?",
        "choices": {
          "A": "Number of rows",
          "B": "Number of columns",
          "C": "Number of colour channels",
          "D": "Bit depth"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "If img.dtype is uint8, what is the possible intensity range for each channel?",
        "choices": {
          "A": "[0, 1]",
          "B": "[-1, 1]",
          "C": "[0, 255]",
          "D": "[0, 1024]"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "What happens if you add 100 to a uint8 image channel without converting to float?",
        "choices": {
          "A": "Values automatically rescale to [0,1]",
          "B": "Values larger than 255 wrap around using modulo arithmetic",
          "C": "Python raises an error",
          "D": "Values are clipped to 255"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Why is it recommended to convert images to float before filtering operations?",
        "choices": {
          "A": "Float images require less memory",
          "B": "Some operations are accurate only using floating point types",
          "C": "Matplotlib cannot display uint8 images",
          "D": "Float images avoid interpolation"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Which function should be used to convert an image to float safely in skimage?",
        "choices": {
          "A": "astype(float)",
          "B": "np.float()",
          "C": "img_as_float",
          "D": "float(img)"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Why should numpy's astype() not be used for image type conversion?",
        "choices": {
          "A": "It changes spatial resolution",
          "B": "It does not properly rescale intensity ranges",
          "C": "It converts RGB to grayscale",
          "D": "It removes channels"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "When plotting a grayscale image with plt.imshow, what should be specified?",
        "choices": {
          "A": "cmap='gray'",
          "B": "dtype='gray'",
          "C": "mode='L'",
          "D": "channels=1"
        },
        "answer": "A"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "What does rgb2gray do?",
        "choices": {
          "A": "Reduces channels from 3 to 1 and converts to float",
          "B": "Removes brightness information",
          "C": "Converts to uint8",
          "D": "Adds a fourth channel"
        },
        "answer": "A"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "After rgb2gray, what is the typical data type?",
        "choices": {
          "A": "uint8",
          "B": "int16",
          "C": "float64",
          "D": "bool"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "When computing a histogram with plt.hist, why must the image be flattened?",
        "choices": {
          "A": "To reduce memory usage",
          "B": "To convert to grayscale",
          "C": "Because hist expects a 1D array",
          "D": "To avoid clipping"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "What does setting density=True in plt.hist produce?",
        "choices": {
          "A": "Raw pixel counts",
          "B": "Probability density function (PDF)",
          "C": "Cumulative distribution",
          "D": "Gamma correction"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Which function performs contrast stretching in skimage?",
        "choices": {
          "A": "exposure.adjust_gamma",
          "B": "exposure.rescale_intensity",
          "C": "exposure.equalize_hist",
          "D": "transform.resize"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Which function applies gamma correction?",
        "choices": {
          "A": "exposure.adjust_gamma",
          "B": "exposure.rescale_intensity",
          "C": "transform.warp",
          "D": "rgb2hsv"
        },
        "answer": "A"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "When applying an AffineTransform using transform.warp, why must tform.inverse be used?",
        "choices": {
          "A": "Because forward mapping creates colour distortions",
          "B": "Because inverse mapping avoids holes in the output image",
          "C": "Because warp only accepts inverse matrices",
          "D": "Because affine matrices are symmetric"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "How many point correspondences are required to estimate an affine transformation?",
        "choices": {
          "A": "2",
          "B": "3",
          "C": "4",
          "D": "6"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "How many correspondences are required for a projective transformation?",
        "choices": {
          "A": "2",
          "B": "3",
          "C": "4",
          "D": "6"
        },
        "answer": "C"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "What does tform.params provide?",
        "choices": {
          "A": "The transformed image",
          "B": "The affine transformation matrix",
          "C": "The histogram values",
          "D": "The interpolation method"
        },
        "answer": "B"
      },
      {
        "section": "digital-images-and-image-processing",
        "question": "Which function estimates transformation parameters from point correspondences?",
        "choices": {
          "A": "transform.rotate",
          "B": "tform.estimate",
          "C": "np.linalg.solve",
          "D": "exposure.equalize_hist"
        },
        "answer": "B"
      },
        {
          "section": "digital-images-and-image-processing",
          "question": "What does it mean that a brightness transformation is position-independent?",
          "choices": {
            "A": "The transformation depends on spatial coordinates (x,y)",
            "B": "The transformation depends only on pixel intensity, not on its position",
            "C": "The transformation changes spatial resolution",
            "D": "The transformation modifies geometry"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "In the brightness transformation J = f(I), what does f represent?",
          "choices": {
            "A": "A spatial mapping of coordinates",
            "B": "A function mapping input intensities to output intensities",
            "C": "A histogram bin index",
            "D": "A colour channel selector"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Why are (x,y) often dropped in brightness transformation equations?",
          "choices": {
            "A": "Because the image is grayscale",
            "B": "Because the transformation does not depend on spatial position",
            "C": "Because coordinates are unknown",
            "D": "Because warp handles geometry"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Which of the following is an example of a position-independent transformation?",
          "choices": {
            "A": "Rotation",
            "B": "Translation",
            "C": "J = 255 − I",
            "D": "Shear"
          },
          "answer": "C"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "What does the negative transformation J = 255 − I represent?",
          "choices": {
            "A": "A geometric reflection",
            "B": "An inversion of intensity values",
            "C": "A histogram equalisation",
            "D": "A spatial shift"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Geometric transformations primarily modify:",
          "choices": {
            "A": "Pixel intensities only",
            "B": "Colour space",
            "C": "Spatial positions of pixels",
            "D": "Bit depth"
          },
          "answer": "C"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Mathematically, a geometric transformation is written as:",
          "choices": {
            "A": "J = f(I)",
            "B": "x' = T x",
            "C": "I = T J",
            "D": "J = αI + β"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "In x' = T x, what does T represent?",
          "choices": {
            "A": "A colour transformation",
            "B": "A mapping function between coordinates",
            "C": "A histogram bin",
            "D": "A pixel intensity"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Why is inverse mapping (T⁻¹) used when warping images?",
          "choices": {
            "A": "To increase dynamic range",
            "B": "To prevent gaps in the output image",
            "C": "To reduce interpolation",
            "D": "To simplify matrix multiplication"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "What is typically required when using inverse mapping?",
          "choices": {
            "A": "Colour balancing",
            "B": "Interpolation of pixel values",
            "C": "Histogram flattening",
            "D": "Bit-depth conversion"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "An affine transformation preserves which properties?",
          "choices": {
            "A": "Angles and lengths only",
            "B": "Collinearity and parallelism",
            "C": "Colour uniformity",
            "D": "Histogram shape"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "How many degrees of freedom does an affine transformation have?",
          "choices": {
            "A": "3",
            "B": "4",
            "C": "6",
            "D": "8"
          },
          "answer": "C"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "How many point correspondences are required to compute an affine transformation?",
          "choices": {
            "A": "2",
            "B": "3",
            "C": "4",
            "D": "6"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Which transformation requires at least 4 correspondences?",
          "choices": {
            "A": "Rigid",
            "B": "Affine",
            "C": "Projective",
            "D": "Translation"
          },
          "answer": "C"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Why does a projective transformation require more correspondences than affine?",
          "choices": {
            "A": "It has fewer parameters",
            "B": "It has 8 degrees of freedom",
            "C": "It preserves angles",
            "D": "It is linear"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Which of the following cannot be represented as a matrix multiplication?",
          "choices": {
            "A": "Affine transformation",
            "B": "Rigid transformation",
            "C": "Projective transformation",
            "D": "Non-linear transformation"
          },
          "answer": "D"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Radial lens distortion is an example of:",
          "choices": {
            "A": "Affine transformation",
            "B": "Rigid transformation",
            "C": "Non-linear transformation",
            "D": "Translation"
          },
          "answer": "C"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "What does translation do to an image?",
          "choices": {
            "A": "Changes pixel intensities",
            "B": "Shifts all pixel positions by a fixed offset",
            "C": "Scales intensity values",
            "D": "Applies gamma correction"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "What does scaling affect in a geometric transformation?",
          "choices": {
            "A": "Brightness only",
            "B": "Spatial distances between pixels",
            "C": "Colour saturation",
            "D": "Histogram bins"
          },
          "answer": "B"
        },
        {
          "section": "digital-images-and-image-processing",
          "question": "Which transformation preserves angles and lengths?",
          "choices": {
            "A": "Projective",
            "B": "Affine",
            "C": "Rigid",
            "D": "Shear"
          },
          "answer": "C"
        },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What is the main purpose of image filtering?",
            "choices": {
              "A": "To increase pixel resolution",
              "B": "To remove unwanted components or enhance desired ones",
              "C": "To change colour space",
              "D": "To convert analogue signals into digital"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "In window-based image filtering, how is the output pixel J(x,y) computed?",
            "choices": {
              "A": "Using the entire image",
              "B": "Using a neighbourhood defined by a window centred at (x,y)",
              "C": "Using histogram equalisation",
              "D": "Using only the central pixel"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "When is filtering considered linear?",
            "choices": {
              "A": "When the output is the median of the window",
              "B": "When the output is a linear combination of neighbourhood pixel values with fixed weights",
              "C": "When the kernel changes at each position",
              "D": "When using HSV colour space"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "The weights used in linear filtering are typically represented as:",
            "choices": {
              "A": "A histogram",
              "B": "A kernel",
              "C": "A gradient map",
              "D": "A CDF"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What is the act of moving a linear kernel over an image called?",
            "choices": {
              "A": "Histogram equalisation",
              "B": "Convolution",
              "C": "Quantisation",
              "D": "Gamma correction"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Which of the following is NOT a property of convolution?",
            "choices": {
              "A": "Commutative",
              "B": "Distributive",
              "C": "Associative",
              "D": "Non-linear"
            },
            "answer": "D"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What does zero padding do during edge handling?",
            "choices": {
              "A": "Replicates nearest border pixels",
              "B": "Extends the image using zeros",
              "C": "Wraps pixels from the opposite side",
              "D": "Removes border pixels"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What is the purpose of kernel normalisation?",
            "choices": {
              "A": "To increase contrast",
              "B": "To ensure the sum of kernel elements equals 1",
              "C": "To reduce kernel size",
              "D": "To sharpen edges"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Why is it recommended to convert images to float before filtering?",
            "choices": {
              "A": "To increase spatial resolution",
              "B": "To avoid clipping and loss of precision",
              "C": "To change colour space",
              "D": "To reduce noise automatically"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What is the output of a moving average (box) filter?",
            "choices": {
              "A": "Maximum value in window",
              "B": "Median value in window",
              "C": "Average of values in window",
              "D": "Central pixel only"
            },
            "answer": "C"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Increasing the size of a box filter primarily results in:",
            "choices": {
              "A": "Stronger smoothing and loss of detail",
              "B": "Sharper edges",
              "C": "Higher dynamic range",
              "D": "Reduced convolution cost"
            },
            "answer": "A"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "In a Gaussian filter, what controls the amount of blurring?",
            "choices": {
              "A": "Kernel size only",
              "B": "Standard deviation σ",
              "C": "Histogram bins",
              "D": "Image resolution"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Compared to a box filter, a Gaussian filter:",
            "choices": {
              "A": "Assigns equal weights to all pixels",
              "B": "Gives larger weight to the central pixel",
              "C": "Is non-linear",
              "D": "Uses median instead of mean"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "What does a sharpening filter enhance?",
            "choices": {
              "A": "Low-frequency components",
              "B": "Image details (differences from local average)",
              "C": "Colour saturation",
              "D": "Histogram bins"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Which filter is non-linear?",
            "choices": {
              "A": "Gaussian filter",
              "B": "Moving average filter",
              "C": "Median filter",
              "D": "Identity filter"
            },
            "answer": "C"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "The median filter is particularly effective against:",
            "choices": {
              "A": "Motion blur",
              "B": "Salt & pepper noise",
              "C": "Lens distortion",
              "D": "Low contrast"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Edge detection primarily identifies:",
            "choices": {
              "A": "Uniform regions",
              "B": "Locations of significant intensity change",
              "C": "Histogram peaks",
              "D": "Colour transitions only"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Gradient-based edge detection is based on:",
            "choices": {
              "A": "Second derivatives",
              "B": "First derivatives",
              "C": "Median filtering",
              "D": "Histogram equalisation"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Laplacian-based edge detection uses:",
            "choices": {
              "A": "First derivative",
              "B": "Second derivative",
              "C": "Median value",
              "D": "Colour channels"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "In the Canny edge detector, what does non-maximum suppression (NMS) do?",
            "choices": {
              "A": "Applies Gaussian smoothing",
              "B": "Suppresses all gradient values except local maxima along the gradient direction",
              "C": "Enhances histogram contrast",
              "D": "Normalises kernel weights"
            },
            "answer": "B"
          },
          {
            "section": "image-filtering-and-edge-detection",
            "question": "Why is Gaussian smoothing typically applied before gradient-based edge detection?",
            "choices": {
              "A": "To increase contrast",
              "B": "To reduce noise and avoid detecting false edges",
              "C": "To convert to HSV",
              "D": "To normalise intensity"
            },
            "answer": "B"
          },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "What is image matching / alignment / registration?",
              "choices": {
                "A": "Changing the colour space of an image",
                "B": "Identifying the transformation that aligns two or more images",
                "C": "Smoothing an image using Gaussian filtering",
                "D": "Detecting edges using gradients"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "In image registration, the goal is to estimate which type of transformation?",
              "choices": {
                "A": "Only rigid transformations",
                "B": "Rigid, affine, or non-rigid transformations",
                "C": "Only histogram-based transformations",
                "D": "Only colour transformations"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "Matching by pixels estimates a transformation by optimising:",
              "choices": {
                "A": "A similarity metric based on image pixels",
                "B": "The histogram equalisation function",
                "C": "The gradient magnitude",
                "D": "The number of corners"
              },
              "answer": "A"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The optimisation problem for pixel-based matching is written as:",
              "choices": {
                "A": "T* = argmin I",
                "B": "T* = argmax_T Similarity(I1, T(I2))",
                "C": "R = det(M)",
                "D": "λ1 + λ2 = 0"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "Examples of similarity metrics used in pixel-based matching include:",
              "choices": {
                "A": "Sobel and Laplacian",
                "B": "Sum of Squared Differences (SSD) and Mutual Information (MI)",
                "C": "Histogram equalisation and gamma correction",
                "D": "Median and Gaussian filtering"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "Matching by edges performs alignment using:",
              "choices": {
                "A": "Raw pixel intensities",
                "B": "Edge maps extracted from the images",
                "C": "Colour histograms only",
                "D": "Bit depth information"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "Matching by interest points has the advantage of being efficient because:",
              "choices": {
                "A": "It uses all image pixels",
                "B": "It uses only some selected keypoints instead of all pixels",
                "C": "It requires no transformation estimation",
                "D": "It avoids similarity metrics"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "An interest point is a point in the image that:",
              "choices": {
                "A": "Has a random intensity value",
                "B": "Has a well-defined position and rich local structure",
                "C": "Is located at the image border",
                "D": "Is always the brightest pixel"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "An interest point should be stable under:",
              "choices": {
                "A": "Colour space conversion only",
                "B": "Local and global perturbations (e.g., illumination variations)",
                "C": "JPEG compression only",
                "D": "Zero padding"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "A corner is defined as:",
              "choices": {
                "A": "A flat region",
                "B": "An intersection of two edges",
                "C": "A single bright pixel",
                "D": "A noise spike"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "In a flat region, intensity change occurs:",
              "choices": {
                "A": "Along both directions",
                "B": "Along one direction only",
                "C": "In no direction",
                "D": "Only diagonally"
              },
              "answer": "C"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "In an edge region, intensity change occurs:",
              "choices": {
                "A": "Along both directions",
                "B": "Along just one direction",
                "C": "In no direction",
                "D": "Randomly"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The matrix M used in the Harris detector is called:",
              "choices": {
                "A": "Covariance matrix",
                "B": "Structure tensor",
                "C": "Homography matrix",
                "D": "Kernel matrix"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The structure tensor M is computed from:",
              "choices": {
                "A": "Second derivatives only",
                "B": "Local first derivatives Ix and Iy",
                "C": "Histogram bins",
                "D": "Colour channels only"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "Eigen decomposition of M allows interpretation in terms of:",
              "choices": {
                "A": "Pixel coordinates",
                "B": "Eigenvalues λ1 and λ2",
                "C": "Histogram bins",
                "D": "Kernel size"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "If λ1 ≈ 0 and λ2 ≈ 0, the region is:",
              "choices": {
                "A": "Corner",
                "B": "Edge",
                "C": "Flat",
                "D": "Blob"
              },
              "answer": "C"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "If λ1 ≫ λ2, the region is:",
              "choices": {
                "A": "Flat",
                "B": "Edge",
                "C": "Corner",
                "D": "Noise"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "If λ1 ≈ λ2 and both are large, the region is:",
              "choices": {
                "A": "Flat",
                "B": "Edge",
                "C": "Corner",
                "D": "Histogram peak"
              },
              "answer": "C"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The Harris cornerness measure is defined as:",
              "choices": {
                "A": "R = λ1 + λ2",
                "B": "R = λ1λ2 − k(λ1 + λ2)^2",
                "C": "R = min(λ1, λ2)",
                "D": "R = λ1 − λ2"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The Harris response can be written without eigen decomposition as:",
              "choices": {
                "A": "R = trace(M)",
                "B": "R = det(M) − k(trace(M))^2",
                "C": "R = Ix + Iy",
                "D": "R = λ1λ2"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The Kanade–Tomasi cornerness measure is defined as:",
              "choices": {
                "A": "R = λ1 + λ2",
                "B": "R = min(λ1, λ2)",
                "C": "R = λ1 − λ2",
                "D": "R = det(M)"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The Noble cornerness measure is defined as:",
              "choices": {
                "A": "R = λ1λ2 / (λ1 + λ2 + ε)",
                "B": "R = λ1 + λ2",
                "C": "R = min(λ1, λ2)",
                "D": "R = trace(M)"
              },
              "answer": "A"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "The first step of the Harris detector is:",
              "choices": {
                "A": "Thresholding",
                "B": "Compute image derivatives Ix and Iy",
                "C": "Eigen decomposition",
                "D": "Match descriptors"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "After computing the Harris response R, the next key step is:",
              "choices": {
                "A": "Colour conversion",
                "B": "Thresholding and non-maximum suppression",
                "C": "Histogram equalisation",
                "D": "Gaussian blur"
              },
              "answer": "B"
            },
            {
              "section": "image-matching-interest-point-detection-and-feature-descriptors",
              "question": "A feature descriptor is:",
              "choices": {
                "A": "A transformation matrix",
                "B": "A representation of the local image neighbourhood around a keypoint",
                "C": "A colour space",
                "D": "A histogram equalisation method"
              },
              "answer": "B"
            },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The limitation of the basic Harris detector is that it is not invariant to:",
                "choices": {
                  "A": "Translation",
                  "B": "Rotation",
                  "C": "Scale changes",
                  "D": "Intensity shifts"
                },
                "answer": "C"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The scale-adapted Harris detector addresses invariance to:",
                "choices": {
                  "A": "Colour changes",
                  "B": "Scale changes",
                  "C": "Histogram stretching",
                  "D": "JPEG compression"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Scale selection in scale-adapted detectors is achieved by:",
                "choices": {
                  "A": "Using a fixed window size",
                  "B": "Analysing the response across multiple scales",
                  "C": "Applying histogram equalisation",
                  "D": "Using only Sobel filters"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The Laplacian-based detector relies on:",
                "choices": {
                  "A": "First derivatives only",
                  "B": "Second derivatives (Laplacian operator)",
                  "C": "Median filtering",
                  "D": "Histogram matching"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The Laplacian operator is defined as:",
                "choices": {
                  "A": "Ix + Iy",
                  "B": "Ix^2 + Iy^2",
                  "C": "Ixx + Iyy",
                  "D": "det(M)"
                },
                "answer": "C"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The Scale-Invariant Feature Transform (SIFT) detector is designed to be invariant to:",
                "choices": {
                  "A": "Colour space only",
                  "B": "Scale and rotation",
                  "C": "Bit depth",
                  "D": "JPEG compression only"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "SIFT detects keypoints by searching for extrema in:",
                "choices": {
                  "A": "Histogram bins",
                  "B": "Difference-of-Gaussians (DoG) scale space",
                  "C": "Median filter outputs",
                  "D": "Raw pixel values only"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Difference-of-Gaussians (DoG) is used in SIFT as an approximation of:",
                "choices": {
                  "A": "The gradient magnitude",
                  "B": "The Laplacian of Gaussian",
                  "C": "The median filter",
                  "D": "The homography"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "SIFT assigns an orientation to each keypoint in order to achieve:",
                "choices": {
                  "A": "Colour invariance",
                  "B": "Rotation invariance",
                  "C": "Scale invariance only",
                  "D": "Histogram invariance"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "A SIFT descriptor represents:",
                "choices": {
                  "A": "A global histogram of the entire image",
                  "B": "Local gradient orientation histograms around a keypoint",
                  "C": "Raw pixel intensities only",
                  "D": "Colour channel averages"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The dimensionality of a standard SIFT descriptor is:",
                "choices": {
                  "A": "32",
                  "B": "64",
                  "C": "128",
                  "D": "256"
                },
                "answer": "C"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Speeded-Up Robust Features (SURF) were introduced to:",
                "choices": {
                  "A": "Increase descriptor dimensionality",
                  "B": "Provide a faster alternative to SIFT",
                  "C": "Remove scale invariance",
                  "D": "Replace Harris detector only"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "SURF approximates the Hessian matrix using:",
                "choices": {
                  "A": "Sobel filters",
                  "B": "Box filters and integral images",
                  "C": "Median filters",
                  "D": "Histogram equalisation"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "A feature descriptor is used to:",
                "choices": {
                  "A": "Estimate image resolution",
                  "B": "Describe the local neighbourhood of a keypoint for matching",
                  "C": "Perform thresholding",
                  "D": "Compute eigenvalues"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Feature matching consists of:",
                "choices": {
                  "A": "Comparing descriptor vectors between images",
                  "B": "Matching raw pixel grids directly",
                  "C": "Matching histograms only",
                  "D": "Matching colour spaces"
                },
                "answer": "A"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "After finding correspondences between keypoints, the next step is to:",
                "choices": {
                  "A": "Apply histogram equalisation",
                  "B": "Estimate the transformation that best aligns the images",
                  "C": "Apply Gaussian blur",
                  "D": "Convert to grayscale"
                },
                "answer": "B"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "The transformation estimated from matched keypoints may be:",
                "choices": {
                  "A": "Rigid, affine, or projective",
                  "B": "Only translation",
                  "C": "Only colour-based",
                  "D": "Only histogram-based"
                },
                "answer": "A"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Interest point detection and feature description are separated because:",
                "choices": {
                  "A": "Detection finds locations, description encodes local appearance",
                  "B": "Detection performs matching",
                  "C": "Description finds corners",
                  "D": "They are mathematically identical"
                },
                "answer": "A"
              },
              {
                "section": "image-matching-interest-point-detection-and-feature-descriptors",
                "question": "Repeatability of a detector refers to:",
                "choices": {
                  "A": "Number of pixels detected",
                  "B": "Ability to detect the same keypoints under different conditions",
                  "C": "Descriptor dimensionality",
                  "D": "Kernel size"
                },
                "answer": "B"
              },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The limitation of the basic Harris detector is that it is not invariant to:",
                  "choices": {
                    "A": "Translation",
                    "B": "Rotation",
                    "C": "Scale changes",
                    "D": "Intensity shifts"
                  },
                  "answer": "C"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The scale-adapted Harris detector addresses invariance to:",
                  "choices": {
                    "A": "Colour changes",
                    "B": "Scale changes",
                    "C": "Histogram stretching",
                    "D": "JPEG compression"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Scale selection in scale-adapted detectors is achieved by:",
                  "choices": {
                    "A": "Using a fixed window size",
                    "B": "Analysing the response across multiple scales",
                    "C": "Applying histogram equalisation",
                    "D": "Using only Sobel filters"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The Laplacian-based detector relies on:",
                  "choices": {
                    "A": "First derivatives only",
                    "B": "Second derivatives (Laplacian operator)",
                    "C": "Median filtering",
                    "D": "Histogram matching"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The Laplacian operator is defined as:",
                  "choices": {
                    "A": "Ix + Iy",
                    "B": "Ix^2 + Iy^2",
                    "C": "Ixx + Iyy",
                    "D": "det(M)"
                  },
                  "answer": "C"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The Scale-Invariant Feature Transform (SIFT) detector is designed to be invariant to:",
                  "choices": {
                    "A": "Colour space only",
                    "B": "Scale and rotation",
                    "C": "Bit depth",
                    "D": "JPEG compression only"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "SIFT detects keypoints by searching for extrema in:",
                  "choices": {
                    "A": "Histogram bins",
                    "B": "Difference-of-Gaussians (DoG) scale space",
                    "C": "Median filter outputs",
                    "D": "Raw pixel values only"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Difference-of-Gaussians (DoG) is used in SIFT as an approximation of:",
                  "choices": {
                    "A": "The gradient magnitude",
                    "B": "The Laplacian of Gaussian",
                    "C": "The median filter",
                    "D": "The homography"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "SIFT assigns an orientation to each keypoint in order to achieve:",
                  "choices": {
                    "A": "Colour invariance",
                    "B": "Rotation invariance",
                    "C": "Scale invariance only",
                    "D": "Histogram invariance"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "A SIFT descriptor represents:",
                  "choices": {
                    "A": "A global histogram of the entire image",
                    "B": "Local gradient orientation histograms around a keypoint",
                    "C": "Raw pixel intensities only",
                    "D": "Colour channel averages"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The dimensionality of a standard SIFT descriptor is:",
                  "choices": {
                    "A": "32",
                    "B": "64",
                    "C": "128",
                    "D": "256"
                  },
                  "answer": "C"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Speeded-Up Robust Features (SURF) were introduced to:",
                  "choices": {
                    "A": "Increase descriptor dimensionality",
                    "B": "Provide a faster alternative to SIFT",
                    "C": "Remove scale invariance",
                    "D": "Replace Harris detector only"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "SURF approximates the Hessian matrix using:",
                  "choices": {
                    "A": "Sobel filters",
                    "B": "Box filters and integral images",
                    "C": "Median filters",
                    "D": "Histogram equalisation"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "A feature descriptor is used to:",
                  "choices": {
                    "A": "Estimate image resolution",
                    "B": "Describe the local neighbourhood of a keypoint for matching",
                    "C": "Perform thresholding",
                    "D": "Compute eigenvalues"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Feature matching consists of:",
                  "choices": {
                    "A": "Comparing descriptor vectors between images",
                    "B": "Matching raw pixel grids directly",
                    "C": "Matching histograms only",
                    "D": "Matching colour spaces"
                  },
                  "answer": "A"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "After finding correspondences between keypoints, the next step is to:",
                  "choices": {
                    "A": "Apply histogram equalisation",
                    "B": "Estimate the transformation that best aligns the images",
                    "C": "Apply Gaussian blur",
                    "D": "Convert to grayscale"
                  },
                  "answer": "B"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "The transformation estimated from matched keypoints may be:",
                  "choices": {
                    "A": "Rigid, affine, or projective",
                    "B": "Only translation",
                    "C": "Only colour-based",
                    "D": "Only histogram-based"
                  },
                  "answer": "A"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Interest point detection and feature description are separated because:",
                  "choices": {
                    "A": "Detection finds locations, description encodes local appearance",
                    "B": "Detection performs matching",
                    "C": "Description finds corners",
                    "D": "They are mathematically identical"
                  },
                  "answer": "A"
                },
                {
                  "section": "image-matching-interest-point-detection-and-feature-descriptors",
                  "question": "Repeatability of a detector refers to:",
                  "choices": {
                    "A": "Number of pixels detected",
                    "B": "Ability to detect the same keypoints under different conditions",
                    "C": "Descriptor dimensionality",
                    "D": "Kernel size"
                  },
                  "answer": "B"
                }
    
]
